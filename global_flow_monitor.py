#!/usr/bin/env python3
"""
ê¸€ë¡œë²Œ í”Œë¡œìš° ëª¨ë‹ˆí„° v2.0
========================
ê¸€ë¡œë²Œ ìê¸ˆ íë¦„ íˆíŠ¸ë§µ ëŒ€ì‹œë³´ë“œ.

v2.0 ë³€ê²½ì‚¬í•­:
    - 5ê°œ ì¹´í…Œê³ ë¦¬ (~47 í‹°ì»¤): US Sector, Regional, Thematic, Risk/Macro, Bonds/FX
    - ê·¸ë£¹ í—¤ë” + í•˜ìœ„ í‹°ì»¤ ê³„ì¸µ êµ¬ì¡°
    - yf.download() ë°°ì¹˜ ìˆ˜ì§‘ (ìˆœì°¨ 60s â†’ ë°°ì¹˜ 10-15s)
    - íˆíŠ¸ë§µ ìŠ¤íƒ€ì¼ JSON v2 â†’ HTML ëŒ€ì‹œë³´ë“œ ì—°ë™
    - ìƒˆ ì‹œê·¸ë„: Credit Spread (HYG-LQD), US Exceptionalism (SPY vs EFA)
    - ì½˜ì†”: ê·¸ë£¹ í‰ê·  ì„œë¨¸ë¦¬ í…Œì´ë¸” (--detail ë¡œ ê°œë³„ í‹°ì»¤)

v1.4 ë³€ê²½ì‚¬í•­:
    - DI-04: --export ì‹œ JSON íŒŒì¼ ìƒì„± â†’ HTML ëŒ€ì‹œë³´ë“œ ìë™ ì—°ë™
    - IA-08: pykrx ê¸°ë°˜ ê°œì¸ ë§¤ìˆ˜ íš¨ìœ¨(Buying Efficiency) ìë™ ê³„ì‚°

ì„¤ì¹˜:
    pip install yfinance pandas tabulate requests

ì‹¤í–‰:
    python global_flow_monitor.py              # ì „ì²´ ëŒ€ì‹œë³´ë“œ (ê·¸ë£¹ ì„œë¨¸ë¦¬)
    python global_flow_monitor.py --detail     # ê°œë³„ í‹°ì»¤ ì „ì²´ ì¶œë ¥
    python global_flow_monitor.py --export     # JSON v2 + CSV ë‚´ë³´ë‚´ê¸°
"""

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
from tabulate import tabulate
import argparse
import sys
import os
import json

# Windows cp949 ì½˜ì†” ì´ëª¨ì§€ ê¹¨ì§ ë°©ì§€
if sys.stdout and hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass

# IA-08: pykrx â€” optional (ê°œì¸ ë§¤ìˆ˜ íš¨ìœ¨ ê³„ì‚°ìš©)
try:
    from pykrx import stock as krx_stock
    HAS_PYKRX = True
except ImportError:
    HAS_PYKRX = False

# ============================================================
# ì„¤ì •: 5ê°œ ì¹´í…Œê³ ë¦¬ í‹°ì»¤ êµ¬ì„± (v2.0)
# ============================================================

CATEGORIES = {
    'us_sectors': {
        'label': 'US Sectors (GICS 11)',
        'icon': 'ğŸ›ï¸',
        'groups': {
            'Tech': {
                'XLK': {'name': 'Technology'},
                'XLC': {'name': 'Communication'},
            },
            'Cyclical': {
                'XLF': {'name': 'Financials'},
                'XLY': {'name': 'Consumer Disc.'},
                'XLI': {'name': 'Industrials'},
            },
            'Defensive': {
                'XLV': {'name': 'Health Care'},
                'XLP': {'name': 'Consumer Staples'},
                'XLU': {'name': 'Utilities'},
            },
            'Commodity': {
                'XLE': {'name': 'Energy'},
                'XLB': {'name': 'Materials'},
            },
            'Rate-Sensitive': {
                'XLRE': {'name': 'Real Estate'},
            },
        },
    },
    'regional': {
        'label': 'Regional Flow',
        'icon': 'ğŸŒ',
        'groups': {
            'DM Americas': {
                'SPY': {'name': 'S&P 500'},
                'EWC': {'name': 'MSCI Canada'},
            },
            'DM Europe': {
                'VGK': {'name': 'FTSE Europe'},
                'EWU': {'name': 'MSCI UK'},
                'EWG': {'name': 'MSCI Germany'},
            },
            'DM Asia-Pac': {
                'EWJ': {'name': 'MSCI Japan'},
                'EWA': {'name': 'MSCI Australia'},
                'EWH': {'name': 'MSCI Hong Kong'},
                'EWS': {'name': 'MSCI Singapore'},
            },
            'EM Asia': {
                'FXI': {'name': 'China Large Cap'},
                'EWY': {'name': 'MSCI Korea'},
                'EWT': {'name': 'MSCI Taiwan'},
                'INDA': {'name': 'MSCI India'},
                'EIDO': {'name': 'MSCI Indonesia'},
            },
            'EM Latam': {
                'EWZ': {'name': 'MSCI Brazil'},
                'EWW': {'name': 'MSCI Mexico'},
            },
            'EM Broad': {
                'EEM': {'name': 'iShares EM'},
                'VWO': {'name': 'Vanguard EM'},
            },
            'DM Broad': {
                'EFA': {'name': 'EAFE (DM ex-US)'},
            },
        },
    },
    'thematic': {
        'label': 'Thematic / Growth',
        'icon': 'ğŸš€',
        'groups': {
            'Semiconductor': {
                'SMH': {'name': 'VanEck Semi'},
                'SOXX': {'name': 'iShares Semi'},
            },
            'AI / Robotics': {
                'BOTZ': {'name': 'Global Robotics & AI'},
                'IRBO': {'name': 'iShares Robotics & AI'},
            },
            'Clean Energy': {
                'ICLN': {'name': 'Global Clean Energy'},
                'TAN': {'name': 'Solar'},
            },
            'Biotech': {
                'XBI': {'name': 'Biotech SPDR'},
            },
            'Innovation': {
                'ARKK': {'name': 'ARK Innovation'},
            },
            'China Tech': {
                'KWEB': {'name': 'China Internet'},
            },
        },
    },
    'risk_macro': {
        'label': 'Risk / Macro',
        'icon': 'âš¡',
        'groups': {
            'Volatility': {
                '^VIX': {'name': 'VIX'},
            },
            'Safe Haven': {
                'GLD': {'name': 'Gold'},
            },
            'Commodities': {
                'USO': {'name': 'WTI Oil'},
                'SLV': {'name': 'Silver'},
                'COPX': {'name': 'Copper Miners'},
                'DBA': {'name': 'Agriculture'},
            },
            'FX': {
                'UUP': {'name': 'USD Index'},
                'FXY': {'name': 'Yen ETF'},
                'FXE': {'name': 'Euro ETF'},
            },
            'Credit': {
                'HYG': {'name': 'High Yield Corp'},
                'LQD': {'name': 'Inv Grade Corp'},
            },
            'Leverage': {
                'TQQQ': {'name': 'Nasdaq 3x Bull'},
                'SQQQ': {'name': 'Nasdaq 3x Bear'},
                'SOXL': {'name': 'Semi 3x Bull'},
                'SOXS': {'name': 'Semi 3x Bear'},
                'UPRO': {'name': 'S&P 3x Bull'},
                'UVXY': {'name': 'VIX 1.5x Long'},
                'TMF': {'name': '20Y Bond 3x Bull'},
                'TBT': {'name': '20Y Bond 2x Bear'},
            },
        },
    },
    'bonds_fx': {
        'label': 'Bonds & Rates',
        'icon': 'ğŸ¦',
        'groups': {
            'US Curve': {
                'SHY': {'name': '1-3Y Treasury'},
                'IEF': {'name': '7-10Y Treasury'},
                'TLT': {'name': '20Y+ Treasury'},
            },
            'Inflation': {
                'TIP': {'name': 'TIPS Bond'},
            },
            'Aggregate': {
                'AGG': {'name': 'US Agg Bond'},
                'BND': {'name': 'Total Bond Market'},
            },
            'EM Bonds': {
                'EMB': {'name': 'EM USD Bond'},
            },
        },
    },
}

# í•„ìˆ˜ í‹°ì»¤ â€” ë³µí•© ì‹œê·¸ë„ íŒë‹¨ì— ë°˜ë“œì‹œ í•„ìš”
REQUIRED_TICKERS = [
    'SMH', 'SOXX',                          # ë°˜ë„ì²´ ë¡œí…Œì´ì…˜
    'EWY', 'EEM',                            # EM ìƒëŒ€ ì„±ê³¼
    '^VIX', 'GLD', 'USO', 'UUP', 'FXY',     # ë¦¬ìŠ¤í¬ ì‹œê·¸ë„
    'HYG', 'LQD',                            # í¬ë ˆë”§ ìŠ¤í”„ë ˆë“œ
    'SPY', 'EFA',                            # US Exceptionalism
    'COPX',                                  # ê²½ê¸° ì„ í–‰
    'SHY', 'IEF', 'TLT',                    # ìˆ˜ìµë¥  ì»¤ë¸Œ
    'VGK', 'EWJ', 'EWA',                    # DM ì§€ì—­ë³„
]


def get_all_tickers():
    """CATEGORIESì—ì„œ ì „ì²´ ê³ ìœ  í‹°ì»¤ ëª©ë¡ ì¶”ì¶œ."""
    tickers = {}  # ticker -> {name, category, group}
    for cat_key, cat in CATEGORIES.items():
        for grp_name, grp_tickers in cat['groups'].items():
            for ticker, info in grp_tickers.items():
                if ticker not in tickers:
                    tickers[ticker] = {
                        'name': info['name'],
                        'category': cat_key,
                        'group': grp_name,
                        'memberships': [],
                    }
                tickers[ticker]['memberships'].append((cat_key, grp_name))
    return tickers


def check_required_tickers(fetched_set):
    """í•„ìˆ˜ í‹°ì»¤ ìˆ˜ì§‘ ì—¬ë¶€ í™•ì¸."""
    return [t for t in REQUIRED_TICKERS if t not in fetched_set]


def print_missing_warning(missing):
    """í•„ìˆ˜ í‹°ì»¤ ëˆ„ë½ ì‹œ ìƒë‹¨ ê²½ê³ ."""
    if not missing:
        return
    width = 70
    print("\n" + "!" * width)
    print("  â›” í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ ê²½ê³  â€” ë³µí•© ì‹œê·¸ë„ ì‹ ë¢°ë„ ì €í•˜")
    print("!" * width)
    for t in missing:
        print(f"  âŒ {t} â€” ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
    print("  â†’ í•´ë‹¹ í‹°ì»¤ì— ì˜ì¡´í•˜ëŠ” ì‹œê·¸ë„ì´ í‰ê°€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("!" * width)


# ============================================================
# ë°ì´í„° ìˆ˜ì§‘ â€” ë°°ì¹˜ (v2.0)
# ============================================================

def fetch_all_data(days=400):
    """yf.download() ë°°ì¹˜ ìˆ˜ì§‘ â†’ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í•  ë°˜í™˜.

    Returns:
        (category_data, errors, meta)
        category_data: {cat_key: DataFrame}  â€” ê° DFì— Ticker,Name,Group,1D%,1W%,1M%,3M%,6M%,1Y%,...
        errors: list of failed tickers
        meta: {'last_date', 'first_date', 'd5_refs'}
    """
    all_tickers = get_all_tickers()
    ticker_list = list(all_tickers.keys())

    end = datetime.now()
    start = end - timedelta(days=days)

    meta = {'last_date': None, 'first_date': None, 'd5_refs': {}}
    errors = []

    # ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ
    print(f"  ğŸ“¡ {len(ticker_list)}ê°œ í‹°ì»¤ ë°°ì¹˜ ìˆ˜ì§‘ ì¤‘...")
    try:
        raw = yf.download(
            ticker_list,
            start=start.strftime('%Y-%m-%d'),
            end=end.strftime('%Y-%m-%d'),
            group_by='ticker',
            progress=False,
            threads=True,
        )
    except Exception as e:
        print(f"  âš ï¸  ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}, ticker_list, meta

    # ê°œë³„ í‹°ì»¤ ì²˜ë¦¬
    rows = []  # (ticker, row_dict)
    for ticker in ticker_list:
        try:
            # ë‹¨ì¼ í‹°ì»¤ì¼ ë•Œì™€ ë³µìˆ˜ í‹°ì»¤ì¼ ë•Œ ì»¬ëŸ¼ êµ¬ì¡°ê°€ ë‹¤ë¦„
            if len(ticker_list) == 1:
                hist = raw
            else:
                if ticker not in raw.columns.get_level_values(0):
                    errors.append(ticker)
                    continue
                hist = raw[ticker].dropna(how='all')

            if hist is None or len(hist) < 2:
                errors.append(ticker)
                continue

            close = hist['Close'].dropna()
            if len(close) < 2:
                errors.append(ticker)
                continue

            last = close.iloc[-1]
            prev = close.iloc[-2]
            first = close.iloc[0]

            # ë‚ ì§œ ì¶”ì 
            last_date = close.index[-1]
            if meta['last_date'] is None or last_date > meta['last_date']:
                meta['last_date'] = last_date
            first_date = close.index[0]
            if meta['first_date'] is None or first_date < meta['first_date']:
                meta['first_date'] = first_date

            # ê¸°ê°„ë³„ ìˆ˜ìµë¥  ê³„ì‚° (trading days ê¸°ì¤€)
            def _ret(n):
                if len(close) > n:
                    ref = close.iloc[-(n+1)]
                    return round((last - ref) / ref * 100, 2)
                return None

            d1_ret = round((last - prev) / prev * 100, 2) if len(close) >= 2 else None
            d5_ret = _ret(5)     # 1W
            d21_ret = _ret(21)   # 1M
            d63_ret = _ret(63)   # 3M
            d126_ret = _ret(126) # 6M
            d252_ret = _ret(252) # 1Y

            # 5D ì°¸ì¡°ì¼
            if len(close) >= 6:
                d5_ref_date = close.index[-6].strftime('%m-%d')
            else:
                d5_ref_date = close.index[0].strftime('%m-%d') + '*'
            meta['d5_refs'][ticker] = d5_ref_date

            # Z-score
            z_5d = None
            if len(close) >= 10:
                rolling_5d = close.pct_change(5) * 100
                rolling_5d = rolling_5d.dropna()
                if len(rolling_5d) >= 5 and rolling_5d.std() > 0:
                    z_5d = round(((d5_ret or 0) - rolling_5d.mean()) / rolling_5d.std(), 2)

            # ê±°ë˜ëŸ‰ ë³€í™” + í‰ê·  ì¼ì¼ ë‹¬ëŸ¬ ê±°ë˜ëŸ‰
            vol = hist.get('Volume')
            vol_change = 0
            avg_dol_vol = 0  # í‰ê·  ì¼ì¼ ë‹¬ëŸ¬ ê±°ë˜ëŸ‰ ($)
            if vol is not None and len(vol) >= 5:
                vol_recent = vol.iloc[-5:].mean()
                close_recent = close.iloc[-5:].mean()
                avg_dol_vol = round(float(vol_recent * close_recent), 0)
                vol_prior = vol.iloc[-10:-5].mean() if len(vol) >= 10 else vol_recent
                if vol_prior > 0:
                    vol_change = round((vol_recent - vol_prior) / vol_prior * 100, 0)
            elif vol is not None and len(vol) >= 2:
                avg_dol_vol = round(float(vol.iloc[-2:].mean() * close.iloc[-2:].mean()), 0)

            info = all_tickers[ticker]
            row = {
                'Ticker': ticker,
                'Name': info['name'],
                'Last': round(float(last), 2),
                '1D %': d1_ret,
                '1W %': d5_ret,
                '1M %': d21_ret,
                '3M %': d63_ret,
                '6M %': d126_ret,
                '1Y %': d252_ret,
                '5D Ref': d5_ref_date,
                '5D Z': z_5d,
                'Vol Î”%': vol_change,
                'AvgDolVol': avg_dol_vol,
            }
            rows.append((ticker, row))

        except Exception as e:
            errors.append(f"{ticker}: {str(e)[:50]}")

    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í• 
    category_data = {}
    for cat_key, cat in CATEGORIES.items():
        cat_rows = []
        for grp_name, grp_tickers in cat['groups'].items():
            for ticker in grp_tickers:
                for t, row in rows:
                    if t == ticker:
                        r = dict(row)
                        r['Group'] = grp_name
                        cat_rows.append(r)
                        break
        if cat_rows:
            category_data[cat_key] = pd.DataFrame(cat_rows)

    # ì¼ê°„ Close + Volume ì‹œê³„ì—´ ë³´ì¡´ (TE ê³„ì‚°ìš©)
    close_series = {}
    volume_series = {}
    for ticker in ticker_list:
        try:
            if len(ticker_list) == 1:
                hist = raw
            else:
                if ticker not in raw.columns.get_level_values(0):
                    continue
                hist = raw[ticker].dropna(how='all')
            if hist is not None and len(hist) >= 2:
                close_series[ticker] = hist['Close'].dropna()
                if 'Volume' in hist.columns:
                    vol = hist['Volume'].dropna()
                    if len(vol) >= 2:
                        volume_series[ticker] = vol
        except Exception:
            pass

    meta['close_series'] = close_series
    meta['volume_series'] = volume_series
    return category_data, errors, meta


# ============================================================
# ê·¸ë£¹ ì¼ê°„ ìˆ˜ìµë¥  + Transfer Entropy
# ============================================================

def build_group_returns(close_series, window=252):
    """í‹°ì»¤ë³„ Close â†’ ê·¸ë£¹ í‰ê·  ì¼ê°„ ìˆ˜ìµë¥  DataFrame.

    Returns:
        pd.DataFrame â€” columns=ê·¸ë£¹ëª…, index=ë‚ ì§œ, values=ì¼ê°„ ìˆ˜ìµë¥ 
    """
    # Bull+Bear í‰ê·  â†’ ë…¸ì´ì¦ˆ. TE ë¶„ì„ì—ì„œ ì˜ë¯¸ ì—†ëŠ” ê·¸ë£¹ ì œì™¸.
    TE_EXCLUDE_GROUPS = {'Leverage', 'Volatility'}

    # í‹°ì»¤ â†’ ê·¸ë£¹ ë§¤í•‘
    ticker_to_group = {}
    for cat_key, cat in CATEGORIES.items():
        for grp_name, grp_tickers in cat['groups'].items():
            if grp_name in TE_EXCLUDE_GROUPS:
                continue
            for ticker in grp_tickers:
                ticker_to_group[ticker] = grp_name

    # í‹°ì»¤ë³„ ì¼ê°„ ìˆ˜ìµë¥ 
    ret_frames = {}
    for ticker, close in close_series.items():
        grp = ticker_to_group.get(ticker)
        if grp is None or len(close) < 10:
            continue
        ret = close.pct_change().dropna()
        if grp not in ret_frames:
            ret_frames[grp] = []
        ret_frames[grp].append(ret)

    # ê·¸ë£¹ í‰ê· 
    group_returns = {}
    for grp, rets in ret_frames.items():
        combined = pd.concat(rets, axis=1).mean(axis=1)
        group_returns[grp] = combined

    df = pd.DataFrame(group_returns).dropna()
    if len(df) > window:
        df = df.iloc[-window:]
    return df


def build_group_flow_returns(close_series, volume_series, window=252):
    """Dollar volume (Close Ã— Volume) ë³€í™”ìœ¨ â†’ ê·¸ë£¹ í‰ê· . ìê¸ˆ íë¦„ proxy.

    Returns:
        pd.DataFrame â€” columns=ê·¸ë£¹ëª…, index=ë‚ ì§œ, values=dollar volume ì¼ê°„ ë³€í™”ìœ¨
    """
    TE_EXCLUDE_GROUPS = {'Leverage', 'Volatility'}

    ticker_to_group = {}
    for cat_key, cat in CATEGORIES.items():
        for grp_name, grp_tickers in cat['groups'].items():
            if grp_name in TE_EXCLUDE_GROUPS:
                continue
            for ticker in grp_tickers:
                ticker_to_group[ticker] = grp_name

    ret_frames = {}
    for ticker, close in close_series.items():
        grp = ticker_to_group.get(ticker)
        vol = volume_series.get(ticker)
        if grp is None or vol is None or len(close) < 10:
            continue
        # Dollar volume = Close Ã— Volume
        common_idx = close.index.intersection(vol.index)
        if len(common_idx) < 10:
            continue
        dv = (close.loc[common_idx] * vol.loc[common_idx])
        # ì¼ê°„ ë³€í™”ìœ¨ (log returnìœ¼ë¡œ ì•ˆì •í™”)
        import numpy as np
        dv_ret = np.log(dv / dv.shift(1)).replace([np.inf, -np.inf], np.nan).dropna()
        if len(dv_ret) < 10:
            continue
        if grp not in ret_frames:
            ret_frames[grp] = []
        ret_frames[grp].append(dv_ret)

    group_returns = {}
    for grp, rets in ret_frames.items():
        combined = pd.concat(rets, axis=1).mean(axis=1)
        group_returns[grp] = combined

    df = pd.DataFrame(group_returns).dropna()
    if len(df) > window:
        df = df.iloc[-window:]
    return df


def compute_group_te(group_returns, bins=10, max_lag=3, n_surrogates=50, alpha=0.05, top_n=15):
    """ê·¸ë£¹ ê°„ Transfer Entropy ê³„ì‚° (ê²½ëŸ‰ ë²„ì „).

    Returns:
        list of dict: [{'src': A, 'tgt': B, 'net_z': float, 'direction': 'Aâ†’B', 'best_lag': int}, ...]
    """
    import numpy as np

    assets = list(group_returns.columns)
    n_assets = len(assets)
    T = len(group_returns)

    if n_assets < 2 or T < 10:
        return []

    # Quantile ì´ì‚°í™”
    discretized = np.zeros((n_assets, T), dtype=int)
    for i, col in enumerate(assets):
        vals = group_returns[col].values
        edges = np.percentile(vals, np.linspace(0, 100, bins + 1))
        edges[-1] += 1e-10
        discretized[i] = np.minimum(np.digitize(vals, edges[1:]), bins - 1)

    def _te_matrix(disc, lag):
        n, t = disc.shape
        nn = t - lag
        results = np.zeros((n, n))
        for tgt in range(n):
            tgt_f = disc[tgt, lag:]
            tgt_p = disc[tgt, :-lag]
            jyy = np.zeros((bins, bins))
            for tt in range(nn):
                jyy[tgt_f[tt], tgt_p[tt]] += 1
            jyy /= nn
            my = np.bincount(tgt_p[:nn], minlength=bins).astype(float) / nn

            for src in range(n):
                if src == tgt:
                    continue
                src_p = disc[src, :-lag]
                jyyx = np.zeros((bins, bins, bins))
                for tt in range(nn):
                    jyyx[tgt_f[tt], tgt_p[tt], src_p[tt]] += 1
                jyyx /= nn
                myx = np.zeros((bins, bins))
                for tt in range(nn):
                    myx[tgt_p[tt], src_p[tt]] += 1
                myx /= nn

                te = 0.0
                for yt in range(bins):
                    for yp in range(bins):
                        for xp in range(bins):
                            p = jyyx[yt, yp, xp]
                            if p > 1e-10 and jyy[yt, yp] > 1e-10 and myx[yp, xp] > 1e-10 and my[yp] > 1e-10:
                                te += p * np.log2((p * my[yp]) / (myx[yp, xp] * jyy[yt, yp]))
                results[src, tgt] = max(0, te)
        return results

    # Best lag scan
    best_te = np.zeros((n_assets, n_assets))
    best_lags = np.ones((n_assets, n_assets), dtype=int)
    for lag in range(1, max_lag + 1):
        te_m = _te_matrix(discretized, lag)
        improved = te_m > best_te
        best_te[improved] = te_m[improved]
        best_lags[improved] = lag

    # Surrogates
    surr_te = np.zeros((n_surrogates, n_assets, n_assets))
    for s in range(n_surrogates):
        perm = np.random.permutation(T)
        shuffled = discretized[:, perm]
        surr_best = np.zeros((n_assets, n_assets))
        for lag in range(1, max_lag + 1):
            sm = _te_matrix(shuffled, lag)
            improved = sm > surr_best
            surr_best[improved] = sm[improved]
        surr_te[s] = surr_best

    te_mean = surr_te.mean(axis=0)
    te_std = surr_te.std(axis=0) + 1e-10
    te_z = (best_te - te_mean) / te_std

    # Net flow for unique pairs
    results = []
    for i in range(n_assets):
        for j in range(i + 1, n_assets):
            z_ij = te_z[i, j]  # iâ†’j
            z_ji = te_z[j, i]  # jâ†’i
            net_z = z_ij - z_ji

            if abs(net_z) < 1.5:
                continue  # ì•½í•œ íë¦„ í•„í„°

            if net_z > 0:
                direction = f'{assets[i]}â†’{assets[j]}'
                leader, follower = assets[i], assets[j]
            else:
                direction = f'{assets[j]}â†’{assets[i]}'
                leader, follower = assets[j], assets[i]
                net_z = -net_z

            lag_val = int(best_lags[i, j] if net_z > 0 else best_lags[j, i])

            results.append({
                'leader': leader,
                'follower': follower,
                'direction': direction,
                'net_z': round(float(net_z), 2),
                'lag': lag_val,
            })

    results.sort(key=lambda x: x['net_z'], reverse=True)
    return results[:top_n]


# ============================================================
# ê·¸ë£¹ ì„œë¨¸ë¦¬ (v2.0)
# ============================================================

def compute_top_movers(category_data, n=5):
    """ì „ì²´ í‹°ì»¤ ì¤‘ 5D% ê¸°ì¤€ top/bottom N ì¶”ì¶œ.

    ì œì™¸: ^VIX (ì§€í‘œ), SOXL/SOXS (ë ˆë²„ë¦¬ì§€/ì¸ë²„ìŠ¤ â€” ì™œê³¡)

    Returns:
        {'gainers': [dict, ...], 'losers': [dict, ...]}
    """
    EXCLUDE = {'^VIX', 'SOXL', 'SOXS', 'TQQQ', 'SQQQ', 'UPRO', 'UVXY', 'TMF', 'TBT'}

    all_rows = []
    # cat_key â†’ (icon, label) ë§¤í•‘
    cat_meta = {k: (v['icon'], v['label']) for k, v in CATEGORIES.items()}

    for cat_key, df in category_data.items():
        icon, label = cat_meta.get(cat_key, ('', ''))
        for _, row in df.iterrows():
            ticker = row['Ticker']
            if ticker in EXCLUDE:
                continue
            d5 = row.get('1W %')
            if d5 is None or pd.isna(d5):
                continue
            def _safe(col):
                v = row.get(col)
                return round(float(v), 2) if v is not None and not pd.isna(v) else None

            all_rows.append({
                'Ticker': ticker,
                'Name': row.get('Name', ''),
                '1D %': _safe('1D %'),
                '1W %': round(float(d5), 2),
                '1M %': _safe('1M %'),
                '3M %': _safe('3M %'),
                '6M %': _safe('6M %'),
                '1Y %': _safe('1Y %'),
                'group': row.get('Group', ''),
                'cat_icon': icon,
            })

    if not all_rows:
        return {'gainers': [], 'losers': []}

    sorted_rows = sorted(all_rows, key=lambda r: r['1W %'], reverse=True)
    gainers = sorted_rows[:n]
    losers = sorted_rows[-n:][::-1]  # worst first (ê°€ì¥ ë‚˜ìœ ê²ƒë¶€í„°)

    return {'gainers': gainers, 'losers': losers}


def compute_group_summaries(category_data):
    """ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹ í‰ê·  ìˆ˜ìµë¥  + ì¶”ì • í¸ì¶œì… ê³„ì‚°.

    Returns:
        {cat_key: {group_name: {
            '1D %': avg, '1W %': avg, '1M %': avg,
            'trading_impact': ì¶”ì • 5D í¸ì¶œì… ($, ì–‘ìˆ˜=ìœ ì…/ìŒìˆ˜=ìœ ì¶œ),
            'avg_dol_vol': ê·¸ë£¹ ì¼í‰ê·  ë‹¬ëŸ¬ê±°ë˜ëŸ‰ í•©ê³„ ($)
        }}}

    est_flow = Î£(ticker_AvgDolVol) Ã— 5 Ã— (group_5D% / 100)
    â†’ ê°€ê²©Ã—ê±°ë˜ëŸ‰ ê¸°ë°˜ ì¶”ì •. ì‹¤ì œ ETF í€ë“œí”Œë¡œìš°ì™€ ë‹¤ë¦„.
    """
    summaries = {}
    for cat_key, df in category_data.items():
        cat_summary = {}
        for grp_name in df['Group'].unique():
            grp = df[df['Group'] == grp_name]
            d5_avg = round(grp['1W %'].mean(), 2)
            grp_dol_vol = grp['AvgDolVol'].sum() if 'AvgDolVol' in grp.columns else 0
            est_flow = round(grp_dol_vol * 5 * (d5_avg / 100), 0)
            def _grp_mean(col):
                s = grp[col].dropna() if col in grp.columns else pd.Series(dtype=float)
                return round(s.mean(), 2) if len(s) > 0 else None

            cat_summary[grp_name] = {
                '1D %': _grp_mean('1D %'),
                '1W %': d5_avg,
                '1M %': _grp_mean('1M %'),
                '3M %': _grp_mean('3M %'),
                '6M %': _grp_mean('6M %'),
                '1Y %': _grp_mean('1Y %'),
                'trading_impact': est_flow,
                'avg_dol_vol': round(grp_dol_vol, 0),
            }
        summaries[cat_key] = cat_summary
    return summaries


# ============================================================
# ì‹œê·¸ë„ (v2.0 â€” ê¸°ì¡´ 4ê°œ + ìƒˆë¡œìš´ 2ê°œ)
# ============================================================

def compute_rotation_score(category_data):
    """ì„¹í„° ë¡œí…Œì´ì…˜ ê°•ë„: Defensive - Tech/Semi.

    v2.0: ìƒˆ ê·¸ë£¹ëª… ê¸°ë°˜. Leverage ê·¸ë£¹ ì œì™¸.
    """
    # Semiconductor from thematic
    thematic = category_data.get('thematic')
    if thematic is None:
        return None

    semi = thematic[thematic['Group'] == 'Semiconductor']
    # Defensive from us_sectors
    sectors = category_data.get('us_sectors')
    if sectors is None:
        return None

    defensive = sectors[sectors['Group'] == 'Defensive']

    if len(semi) == 0 or len(defensive) == 0:
        return None

    semi_avg = semi['1W %'].mean()
    defensive_avg = defensive['1W %'].mean()
    score = round(defensive_avg - semi_avg, 2)

    # Leverage í¬í•¨ ë¹„êµ (ê²€ì¦ìš© â€” SOXLë§Œ semi ê´€ë ¨)
    risk_data = category_data.get('risk_macro')
    score_meta = None
    if risk_data is not None:
        soxl = risk_data[risk_data['Ticker'] == 'SOXL']
        if len(soxl) > 0:
            import pandas as pd
            semi_with_soxl = pd.concat([semi, soxl])
            raw = round(defensive_avg - semi_with_soxl['1W %'].mean(), 2)
            dev = abs(raw - score) / abs(score) * 100 if score != 0 else 0
            score_meta = {'raw_with_leverage': raw, 'deviation_pct': round(dev, 0)}

    return score, score_meta


def compute_em_relative(category_data):
    """EWY vs EEM ìƒëŒ€ ì„±ê³¼ (IA-03 ë³´ì • ìœ ì§€)."""
    EWY_WEIGHT_IN_EEM = 0.12
    regional = category_data.get('regional')
    if regional is None:
        return None

    korea = regional[regional['Ticker'] == 'EWY']
    em_broad = regional[regional['Ticker'] == 'EEM']
    taiwan = regional[regional['Ticker'] == 'EWT']

    relatives = []
    if len(korea) > 0 and len(em_broad) > 0:
        ewy_5d = korea['1W %'].values[0]
        eem_5d = em_broad['1W %'].values[0]
        ewy_30d = korea['1M %'].values[0]
        eem_30d = em_broad['1M %'].values[0]

        eem_ex_kr_5d = (eem_5d - ewy_5d * EWY_WEIGHT_IN_EEM) / (1 - EWY_WEIGHT_IN_EEM)
        eem_ex_kr_30d = (eem_30d - ewy_30d * EWY_WEIGHT_IN_EEM) / (1 - EWY_WEIGHT_IN_EEM)

        diff_5d_raw = ewy_5d - eem_5d
        diff_5d_adj = ewy_5d - eem_ex_kr_5d
        diff_30d_adj = ewy_30d - eem_ex_kr_30d

        interp = 'í•œêµ­ EM ëŒ€ë¹„ ê°•ì„¸' if diff_5d_adj > 0 else 'í•œêµ­ EM ëŒ€ë¹„ ì•½ì„¸'
        relatives.append({
            'Pair': 'EWY vs EEM(ë³´ì •)',
            '1W ìƒëŒ€%': round(diff_5d_adj, 2),
            '1M ìƒëŒ€%': round(diff_30d_adj, 2),
            'í•´ì„': f'{interp} (ë³´ì • ì „ {diff_5d_raw:+.1f}%)'
        })

    if len(korea) > 0 and len(taiwan) > 0:
        diff_5d = korea['1W %'].values[0] - taiwan['1W %'].values[0]
        diff_30d = korea['1M %'].values[0] - taiwan['1M %'].values[0]
        relatives.append({
            'Pair': 'EWY vs EWT',
            '1W ìƒëŒ€%': round(diff_5d, 2),
            '1M ìƒëŒ€%': round(diff_30d, 2),
            'í•´ì„': 'í•œêµ­ ëŒ€ë§Œ ëŒ€ë¹„ ê°•ì„¸' if diff_5d > 0 else 'í•œêµ­ ëŒ€ë§Œ ëŒ€ë¹„ ì•½ì„¸ (ë°˜ë„ì²´ ë¹„ì¤‘â†‘)'
        })

    return pd.DataFrame(relatives) if relatives else None


def _get_ticker_row(category_data, ticker):
    """ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì—ì„œ íŠ¹ì • í‹°ì»¤ í–‰ ì°¾ê¸°."""
    for df in category_data.values():
        match = df[df['Ticker'] == ticker]
        if len(match) > 0:
            return match.iloc[0]
    return None


def compute_risk_dashboard(category_data):
    """ë¦¬ìŠ¤í¬ ìƒíƒœ ì¢…í•© íŒë‹¨ â€” Z-score ì ì‘í˜•."""
    risk_df = category_data.get('risk_macro')
    if risk_df is None:
        return []

    signals = []
    for _, row in risk_df.iterrows():
        ticker = row['Ticker']
        d5 = row['1W %']
        z = row.get('5D Z')
        z_tag = f" Z={z:+.1f}" if z is not None else ""

        if ticker == '^VIX':
            level = row['Last']
            if z is not None and z > 2:
                signals.append(('ğŸ”´', f"VIX {level:.0f} â€” 30ì¼ ëŒ€ë¹„ ê¸‰ë“±{z_tag}"))
            elif level > 25:
                signals.append(('ğŸ”´', f"VIX {level:.0f} â€” ê³µí¬ êµ¬ê°„{z_tag}"))
            elif z is not None and z > 1:
                signals.append(('ğŸŸ¡', f"VIX {level:.0f} â€” 30ì¼ ëŒ€ë¹„ ìƒìŠ¹{z_tag}"))
            elif level > 20:
                signals.append(('ğŸŸ¡', f"VIX {level:.0f} â€” ê²½ê³„ êµ¬ê°„{z_tag}"))
            else:
                signals.append(('ğŸŸ¢', f"VIX {level:.0f} â€” ì•ˆì •{z_tag}"))

        elif ticker == 'GLD':
            if z is not None and z > 2:
                signals.append(('ğŸ”´', f"ê¸ˆ 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ì´ìƒ ê¸‰ë“±{z_tag}"))
            elif d5 > 2:
                signals.append(('ğŸ”´', f"ê¸ˆ 1W {d5:+.1f}% â€” ì•ˆì „ìì‚° ìˆ˜ìš” ê¸‰ì¦{z_tag}"))
            elif z is not None and z > 1:
                signals.append(('ğŸŸ¡', f"ê¸ˆ 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ìƒìŠ¹{z_tag}"))
            elif d5 > 0.5:
                signals.append(('ğŸŸ¡', f"ê¸ˆ 1W {d5:+.1f}% â€” ì•ˆì „ìì‚° ì†Œí­ ì„ í˜¸{z_tag}"))

        elif ticker == 'USO':
            if z is not None and z > 2:
                signals.append(('ğŸ”´', f"ìœ ê°€ 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ì´ìƒ ê¸‰ë“±{z_tag}"))
            elif d5 > 5:
                signals.append(('ğŸ”´', f"ìœ ê°€ 1W {d5:+.1f}% â€” ì§€ì •í•™/ê³µê¸‰ ë¦¬ìŠ¤í¬{z_tag}"))
            elif z is not None and z < -2:
                signals.append(('ğŸŸ¢', f"ìœ ê°€ 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ì´ìƒ ê¸‰ë½{z_tag}"))
            elif d5 < -5:
                signals.append(('ğŸŸ¢', f"ìœ ê°€ 1W {d5:+.1f}% â€” ìˆ˜ìš” ì•½í™” ìš°ë ¤{z_tag}"))

        elif ticker == 'UUP':
            if z is not None and z > 2:
                signals.append(('ğŸ”´', f"ë‹¬ëŸ¬ 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ì´ìƒ ê°•ì„¸{z_tag}"))
            elif d5 > 1:
                signals.append(('ğŸ”´', f"ë‹¬ëŸ¬ 1W {d5:+.1f}% â€” EM ìê¸ˆìœ ì¶œ ì••ë ¥{z_tag}"))
            elif z is not None and z < -2:
                signals.append(('ğŸŸ¢', f"ë‹¬ëŸ¬ 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ì´ìƒ ì•½ì„¸{z_tag}"))
            elif d5 < -1:
                signals.append(('ğŸŸ¢', f"ë‹¬ëŸ¬ 1W {d5:+.1f}% â€” EM ìê¸ˆìœ ì… ìš°í˜¸ì {z_tag}"))

        elif ticker == 'FXY':
            if z is not None and z > 2:
                signals.append(('ğŸŸ¡', f"ì—”í™” 1W {d5:+.1f}% â€” 30ì¼ ëŒ€ë¹„ ì´ìƒ ê°•ì„¸{z_tag}"))
            elif d5 > 2:
                signals.append(('ğŸŸ¡', f"ì—”í™” 1W {d5:+.1f}% â€” ìºë¦¬íŠ¸ë ˆì´ë“œ ì²­ì‚° ì£¼ì˜{z_tag}"))

    return signals


def generate_composite_signals(category_data):
    """ë³µí•© ì‹œê·¸ë„ ìƒì„± â€” ê¸°ì¡´ 4ê°œ + ìƒˆ 2ê°œ."""
    signals = []

    # Helper
    def get_row(ticker):
        return _get_ticker_row(category_data, ticker)

    # 1. ë°˜ë„ì²´ ë¡œí…Œì´ì…˜ + ë‹¬ëŸ¬ ê°•ì„¸ = í•œêµ­ ë§¤ë„ ì••ë ¥
    rot_result = compute_rotation_score(category_data)
    rot_score = rot_result[0] if rot_result is not None else None
    uup = get_row('UUP')

    if rot_score is not None and rot_score > 3 and uup is not None and uup['1W %'] > 0.5:
        signals.append({
            'Level': 'ğŸ”´ HIGH',
            'Signal': 'ë°˜ë„ì²´ ë¡œí…Œì´ì…˜ + ë‹¬ëŸ¬ ê°•ì„¸ ë™ì‹œ ë°œìƒ',
            'Implication': 'ì™¸ì¸ í•œêµ­ í˜„ë¬¼ ìˆœë§¤ë„ ê°€ì† ì˜ˆìƒ. ì„ ë¬¼ ìˆ ë™ë°˜ ê°€ëŠ¥.',
            'Data': f'ë¡œí…Œì´ì…˜ ìŠ¤ì½”ì–´: {rot_score}, ë‹¬ëŸ¬ 1W: +{uup["1W %"]:.1f}%'
        })

    # 2. EM ìœ ì… + í•œêµ­ ì–¸ë”í¼í¼ = ë°˜ë„ì²´ ê¸°í”¼
    em_rel = compute_em_relative(category_data)
    if em_rel is not None and len(em_rel) > 0:
        eem_row = em_rel[em_rel['Pair'] == 'EWY vs EEM(ë³´ì •)']
        if len(eem_row) > 0 and eem_row['1W ìƒëŒ€%'].values[0] < -2:
            signals.append({
                'Level': 'ğŸŸ¡ MED',
                'Signal': 'EM ìê¸ˆ ìœ ì… ì¤‘ì´ì§€ë§Œ í•œêµ­ì€ ì†Œì™¸',
                'Implication': 'ê¸€ë¡œë²Œ EM ë¡œí…Œì´ì…˜ì—ì„œ í•œêµ­=ë°˜ë„ì²´ ì¸ì‹ìœ¼ë¡œ ë¹„ì¤‘ ì¶•ì†Œ.',
                'Data': f'EWY vs EEM(ë³´ì •) 5D: {eem_row["1W ìƒëŒ€%"].values[0]:+.1f}%'
            })

    # 3. ê¸ˆ+ìœ ê°€ ë™ì‹œ ê¸‰ë“± â€” IA-05 êµì°¨ ë¶„ë¥˜
    gld = get_row('GLD')
    uso = get_row('USO')
    if gld is not None and uso is not None:
        gld_5d = gld['1W %']
        uso_5d = uso['1W %']
        if gld_5d > 1.5 and uso_5d > 3:
            tlt = get_row('TLT')
            uup_r = get_row('UUP')
            fxi = get_row('FXI')
            tlt_5d = tlt['1W %'] if tlt is not None else 0
            uup_5d = uup_r['1W %'] if uup_r is not None else 0
            fxi_5d = fxi['1W %'] if fxi is not None else 0

            cross = f'TLT 1W: {tlt_5d:+.1f}%, USD 1W: {uup_5d:+.1f}%, FXI 1W: {fxi_5d:+.1f}%'
            if tlt_5d > 1 and uup_5d > 0:
                signals.append({
                    'Level': 'ğŸ”´ HIGH',
                    'Signal': 'ê¸ˆ + ìœ ê°€ ë™ì‹œ ìƒìŠ¹ â€” ì§€ì •í•™ ë¦¬ìŠ¤í¬ (TLT ë™ë°˜ ìƒìŠ¹ í™•ì¸)',
                    'Implication': 'ì•ˆì „ìì‚° ë™ë°˜ ìƒìŠ¹ â€” ë¦¬ìŠ¤í¬ì˜¤í”„ í™•ì¸. ì¤‘ë™/ëŒ€ë§Œ ê¸´ì¥ ì ê²€ í•„ìš”.',
                    'Data': f'ê¸ˆ 1W: +{gld_5d:.1f}%, ìœ ê°€ 1W: +{uso_5d:.1f}% | {cross}'
                })
            elif uup_5d < -0.5:
                signals.append({
                    'Level': 'ğŸŸ¡ MED',
                    'Signal': 'ê¸ˆ + ìœ ê°€ ë™ì‹œ ìƒìŠ¹ â€” ë‹¬ëŸ¬ ì•½ì„¸ ì£¼ë„ (ëª…ëª© ìƒìŠ¹)',
                    'Implication': 'ë‹¬ëŸ¬ ì•½ì„¸ê°€ ì›ìì¬ ê°€ê²©ì„ ë°€ì–´ì˜¬ë¦¼. EM ìê¸ˆìœ ì…ì—ëŠ” ìš°í˜¸ì .',
                    'Data': f'ê¸ˆ 1W: +{gld_5d:.1f}%, ìœ ê°€ 1W: +{uso_5d:.1f}% | {cross}'
                })
            elif fxi_5d > 2:
                signals.append({
                    'Level': 'ğŸŸ¡ MED',
                    'Signal': 'ê¸ˆ + ìœ ê°€ ë™ì‹œ ìƒìŠ¹ â€” ì¤‘êµ­ ê²½ê¸°ë¶€ì–‘ ê¸°ëŒ€ (FXI ë™ë°˜ ìƒìŠ¹)',
                    'Implication': 'FXI ê°•ì„¸ ë™ë°˜ â€” ì¤‘êµ­ ë¶€ì–‘ì±… ê¸°ëŒ€. í•œêµ­ ìˆ˜ì¶œì£¼ì— ê¸ì •ì  ê°€ëŠ¥ì„±.',
                    'Data': f'ê¸ˆ 1W: +{gld_5d:.1f}%, ìœ ê°€ 1W: +{uso_5d:.1f}% | {cross}'
                })
            else:
                signals.append({
                    'Level': 'ğŸŸ¡ MED',
                    'Signal': 'ê¸ˆ + ìœ ê°€ ë™ì‹œ ìƒìŠ¹ â€” ì¸í”Œë ˆì´ì…˜ ê¸°ëŒ€ ìƒìŠ¹',
                    'Implication': 'ì•ˆì „ìì‚° ë™ë°˜ ì—†ì´ ì›ìì¬ë§Œ ìƒìŠ¹. ì¸í”Œë ˆ ê¸°ëŒ€ ìš°ì„¸, ê¸ˆë¦¬ ê²½ë¡œ ì£¼ì‹œ.',
                    'Data': f'ê¸ˆ 1W: +{gld_5d:.1f}%, ìœ ê°€ 1W: +{uso_5d:.1f}% | {cross}'
                })

    # 4. VIX ê¸‰ë“± + ì—”í™” ê°•ì„¸ = ìºë¦¬íŠ¸ë ˆì´ë“œ ì²­ì‚°
    vix = get_row('^VIX')
    fxy = get_row('FXY')
    if vix is not None and fxy is not None:
        vix_val = vix['Last']
        fxy_5d = fxy['1W %']
        data_str = f'VIX: {vix_val:.0f}, ì—”í™” 1W: {fxy_5d:+.1f}%'
        if vix_val > 28 and fxy_5d > 3:
            signals.append({
                'Level': 'ğŸ”´ HIGH',
                'Signal': 'VIX ê¸‰ë“± + ì—”í™” ê¸‰ë“± â€” ìºë¦¬íŠ¸ë ˆì´ë“œ ì²­ì‚° ì§„í–‰',
                'Implication': 'ì—” ìºë¦¬ ì²­ì‚° í™•ì¸ ìˆ˜ì¤€. í•œêµ­ í¬í•¨ EM ì „ë°˜ ìê¸ˆ ì´íƒˆ.',
                'Data': data_str
            })
        elif vix_val > 22 and fxy_5d > 1.5:
            signals.append({
                'Level': 'ğŸŸ¡ MED',
                'Signal': 'VIX ìƒìŠ¹ + ì—”í™” ê°•ì„¸ â€” ìºë¦¬íŠ¸ë ˆì´ë“œ ì²­ì‚° ì£¼ì˜',
                'Implication': 'ì•„ì§ ì²­ì‚° í™•ì¸ ìˆ˜ì¤€ ì•„ë‹˜ (HIGH: VIX 28+, FXY +3%). ëª¨ë‹ˆí„°ë§.',
                'Data': data_str
            })

    # 5. [NEW] Credit Spread: HYG-LQD 1W ìŠ¤í”„ë ˆë“œ
    hyg = get_row('HYG')
    lqd = get_row('LQD')
    if hyg is not None and lqd is not None:
        spread_5d = hyg['1W %'] - lqd['1W %']
        if spread_5d < -1.5:
            signals.append({
                'Level': 'ğŸ”´ HIGH',
                'Signal': f'í¬ë ˆë”§ ìŠ¤í”„ë ˆë“œ í™•ëŒ€ â€” HYG vs LQD 1W: {spread_5d:+.1f}%',
                'Implication': 'í•˜ì´ì¼ë“œ ê¸‰ë½ / íˆ¬ìë“±ê¸‰ ìƒëŒ€ ê°•ì„¸ â†’ ì‹ ìš© ë¦¬ìŠ¤í¬ í™•ëŒ€. ìœ„í—˜ ìì‚° ì „ë°˜ ê²½ê³„.',
                'Data': f'HYG 1W: {hyg["1W %"]:+.1f}%, LQD 1W: {lqd["1W %"]:+.1f}%'
            })
        elif spread_5d < -0.8:
            signals.append({
                'Level': 'ğŸŸ¡ MED',
                'Signal': f'í¬ë ˆë”§ ìŠ¤í”„ë ˆë“œ ì†Œí­ í™•ëŒ€ â€” HYG vs LQD 1W: {spread_5d:+.1f}%',
                'Implication': 'í•˜ì´ì¼ë“œ ì•½ì„¸ ì‹œì‘. ì¶”ì„¸ ì§€ì† ì‹œ ë¦¬ìŠ¤í¬ì˜¤í”„ ì „í™˜ ê°€ëŠ¥.',
                'Data': f'HYG 1W: {hyg["1W %"]:+.1f}%, LQD 1W: {lqd["1W %"]:+.1f}%'
            })

    # 6. [NEW] US Exceptionalism: SPY vs EFA
    spy = get_row('SPY')
    efa = get_row('EFA')
    if spy is not None and efa is not None:
        us_ex_5d = spy['1W %'] - efa['1W %']
        if us_ex_5d > 3:
            signals.append({
                'Level': 'ğŸŸ¡ MED',
                'Signal': f'US Exceptionalism â€” SPY vs EFA 1W: +{us_ex_5d:.1f}%',
                'Implication': 'ë¯¸êµ­ ë…ì£¼ â†’ ë¹„ë¯¸êµ­ ìì‚°ì—ì„œ ìê¸ˆ ì´íƒˆ ì••ë ¥. EM ë¡œí…Œì´ì…˜ ë¦¬ìŠ¤í¬.',
                'Data': f'SPY 1W: {spy["1W %"]:+.1f}%, EFA 1W: {efa["1W %"]:+.1f}%'
            })

    # ì‹œê·¸ë„ ë¶€ì¬ ì‹œ ì»¤ë²„ë¦¬ì§€ ëª…ì‹œ
    if not signals:
        signals.append({
            'Level': 'ğŸŸ¢ LOW',
            'Signal': 'ì£¼ìš” ë³µí•© ê²½ê³  ì‹œê·¸ë„ ì—†ìŒ (6/6 ì¡°ê±´ ë¯¸í•´ë‹¹)',
            'Implication': 'ê°ì‹œ ì¤‘ì¸ 6ê°œ ì‹œë‚˜ë¦¬ì˜¤ í•´ë‹¹ ì—†ìŒ. '
                          'ë¯¸ê°ì‹œ ì˜ì—­: ìœ„ì•ˆí™”, í•œêµ­ ì •ì¹˜/ëŒ€ë¶, ê¸€ë¡œë²Œ ìœ ë™ì„±.',
            'Data': ''
        })

    return signals


# ============================================================
# IA-08: ê°œì¸ ë§¤ìˆ˜ íš¨ìœ¨ (Buying Efficiency) â€” ê·¸ëŒ€ë¡œ ìœ ì§€
# ============================================================

def compute_buying_efficiency(date_str=None):
    """KRX ë°ì´í„° ê¸°ë°˜ ê°œì¸ ë§¤ìˆ˜ íš¨ìœ¨ ê³„ì‚°."""
    if not HAS_PYKRX:
        return None
    try:
        if date_str is None:
            today = datetime.now()
            end_dt = today.strftime('%Y%m%d')
            start_dt = (today - timedelta(days=7)).strftime('%Y%m%d')
        else:
            end_dt = date_str.replace('-', '')
            start_dt = (datetime.strptime(end_dt, '%Y%m%d') - timedelta(days=7)).strftime('%Y%m%d')

        kospi = krx_stock.get_index_ohlcv(start_dt, end_dt, "1001")
        if kospi is None or len(kospi) < 2:
            return {'error': 'ì½”ìŠ¤í”¼ ë°ì´í„° ë¶€ì¡±', 'date': end_dt}

        last_date = kospi.index[-1]
        kospi_close = kospi['ì¢…ê°€'].iloc[-1]
        kospi_prev = kospi['ì¢…ê°€'].iloc[-2]
        kospi_change = round(kospi_close - kospi_prev, 2)

        trade_date = last_date.strftime('%Y%m%d')
        trading = krx_stock.get_market_trading_value_by_investor(
            trade_date, trade_date, "KOSPI"
        )
        if trading is None or len(trading) == 0:
            return {'error': 'íˆ¬ìì ë§¤ë§¤ ë°ì´í„° ì—†ìŒ', 'date': trade_date}

        if 'ê°œì¸' in trading.index:
            individual_net = trading.loc['ê°œì¸', 'ìˆœë§¤ìˆ˜']
            individual_net_trillion = round(individual_net / 1_000_000_000_000, 2)
        else:
            return {'error': 'ê°œì¸ íˆ¬ìì ë°ì´í„° ì—†ìŒ', 'date': trade_date}

        if abs(individual_net_trillion) < 0.01:
            efficiency = None
        else:
            efficiency = round(kospi_change / individual_net_trillion, 1)

        foreign_net_trillion = None
        if 'ì™¸êµ­ì¸' in trading.index:
            foreign_net = trading.loc['ì™¸êµ­ì¸', 'ìˆœë§¤ìˆ˜']
            foreign_net_trillion = round(foreign_net / 1_000_000_000_000, 2)

        return {
            'date': last_date.strftime('%Y-%m-%d'),
            'kospi_close': kospi_close,
            'kospi_change': kospi_change,
            'individual_net_buy': individual_net_trillion,
            'foreign_net_buy': foreign_net_trillion,
            'efficiency': efficiency,
            'error': None,
        }
    except Exception as e:
        return {'error': f'KRX ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)[:80]}'}


# ============================================================
# JSON v2 ì¶œë ¥
# ============================================================

def export_json_v2(category_data, summaries, composite, meta, buying_eff=None, group_te=None):
    """ê³„ì¸µì  JSON v2 ì¶œë ¥.

    {version: 2, categories: {cat_key: {label, icon, groups, summary}}, signals, kpi, ...}
    """

    def nan_to_none(val):
        if pd.isna(val):
            return None
        return val

    def row_to_dict(row):
        return {k: nan_to_none(v) for k, v in row.items()}

    data = {
        'version': 2,
        'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M KST'),
        'data_date': None,
        'categories': {},
        'signals': [],
        'kpi': {},
        'buying_efficiency': None,
    }

    # ë°ì´í„° ê¸°ì¤€ì¼
    if meta.get('last_date') is not None:
        ld = meta['last_date']
        weekdays_ko = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        data['data_date'] = ld.strftime('%Y-%m-%d') + f' ({weekdays_ko[ld.weekday()]})'

    # ì¹´í…Œê³ ë¦¬ë³„ êµ¬ì¡°í™”
    for cat_key, cat_config in CATEGORIES.items():
        df = category_data.get(cat_key)
        if df is None:
            continue

        cat_out = {
            'label': cat_config['label'],
            'icon': cat_config['icon'],
            'groups': {},
            'summary': summaries.get(cat_key, {}),
        }

        for grp_name in cat_config['groups']:
            grp_rows = df[df['Group'] == grp_name]
            if len(grp_rows) > 0:
                cat_out['groups'][grp_name] = [row_to_dict(r) for _, r in grp_rows.iterrows()]

        data['categories'][cat_key] = cat_out

    # ë³µí•© ì‹œê·¸ë„
    if composite:
        for s in composite:
            data['signals'].append({
                'level': s.get('Level', ''),
                'signal': s.get('Signal', ''),
                'implication': s.get('Implication', ''),
                'data': s.get('Data', ''),
            })

    # KPI
    rot_result = compute_rotation_score(category_data)
    if rot_result is not None:
        data['kpi']['rotation_score'] = rot_result[0]

    em_rel = compute_em_relative(category_data)
    if em_rel is not None and len(em_rel) > 0:
        eem_row = em_rel[em_rel['Pair'] == 'EWY vs EEM(ë³´ì •)']
        if len(eem_row) > 0:
            data['kpi']['em_relative_5d'] = eem_row['1W ìƒëŒ€%'].values[0]

    vix = _get_ticker_row(category_data, '^VIX')
    if vix is not None:
        data['kpi']['vix'] = float(vix['Last'])

    hyg = _get_ticker_row(category_data, 'HYG')
    lqd = _get_ticker_row(category_data, 'LQD')
    if hyg is not None and lqd is not None:
        data['kpi']['hyg_lqd_spread_5d'] = round(hyg['1W %'] - lqd['1W %'], 2)

    spy = _get_ticker_row(category_data, 'SPY')
    efa = _get_ticker_row(category_data, 'EFA')
    if spy is not None and efa is not None:
        data['kpi']['spy_vs_efa_5d'] = round(spy['1W %'] - efa['1W %'], 2)

    # v1 í˜¸í™˜ ë ˆì´ì–´
    v1 = {'rotation': [], 'em': [], 'risk': []}
    v1_map = {
        'us_sectors': 'rotation',
        'thematic': 'rotation',
        'regional': 'em',
        'risk_macro': 'risk',
        'bonds_fx': 'risk',
    }
    for cat_key, df in category_data.items():
        target = v1_map.get(cat_key, 'risk')
        cols = ['Group', 'Ticker', 'Name', '1D %', '1W %', '1M %', '3M %', '6M %', '1Y %', 'Vol Î”%']
        available = [c for c in cols if c in df.columns]
        records = df[available].to_dict('records')
        for r in records:
            for k, v in r.items():
                if pd.isna(v):
                    r[k] = None
        v1[target].extend(records)
    data['_v1_compat'] = v1

    # Top Movers
    data['top_movers'] = compute_top_movers(category_data, n=5)

    # ë§¤ìˆ˜ íš¨ìœ¨
    if buying_eff is not None and buying_eff.get('error') is None:
        data['buying_efficiency'] = buying_eff

    # Group TE (multi-window)
    if group_te:
        data['group_te'] = group_te

    # ì €ì¥
    filepath = 'flow_monitor_latest.json'
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ’¾ JSON v2: {filepath}")

    return filepath


# ============================================================
# ì½˜ì†” ì¶œë ¥
# ============================================================

def print_header(title, data_date=None):
    width = 70
    print("\n" + "â•" * width)
    print(f"  {title}")
    run_time = datetime.now().strftime('%Y-%m-%d %H:%M KST')
    if data_date is not None:
        weekdays_ko = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        wd = weekdays_ko[data_date.weekday()]
        print(f"  ì‹¤í–‰: {run_time}  |  ë°ì´í„° ê¸°ì¤€: {data_date.strftime('%Y-%m-%d')} ({wd})")
    else:
        print(f"  {run_time}")
    print("â•" * width)


def print_group_summary(summaries):
    """ê·¸ë£¹ í‰ê·  ì„œë¨¸ë¦¬ í…Œì´ë¸” ì¶œë ¥."""
    rows = []
    for cat_key, cat_config in CATEGORIES.items():
        grp_data = summaries.get(cat_key, {})
        for grp_name, vals in grp_data.items():
            rows.append({
                'Category': cat_config['label'][:16],
                'Group': grp_name,
                '1D %': vals['1D %'],
                '1W %': vals['1W %'],
                '1M %': vals['1M %'],
                '3M %': vals.get('3M %'),
                '6M %': vals.get('6M %'),
                '1Y %': vals.get('1Y %'),
            })

    if rows:
        df = pd.DataFrame(rows)
        print(tabulate(df, headers='keys', tablefmt='simple', showindex=False,
                       numalign='right', floatfmt='+.2f'))


def print_detail_tables(category_data):
    """ê°œë³„ í‹°ì»¤ ì „ì²´ ì¶œë ¥ (--detail)."""
    for cat_key, cat_config in CATEGORIES.items():
        df = category_data.get(cat_key)
        if df is None:
            continue
        print(f"\nâ”€â”€ {cat_config['icon']} {cat_config['label']} {'â”€' * 40}")
        cols = ['Group', 'Ticker', 'Name', 'Last', '1D %', '1W %', '1M %', '3M %', '6M %', '1Y %', 'Vol Î”%']
        available = [c for c in cols if c in df.columns]
        print(tabulate(df[available], headers='keys', tablefmt='simple',
                       showindex=False, numalign='right', floatfmt='.2f'))


def print_signals(title, signals):
    print(f"\nâ”€â”€ {title} {'â”€' * max(1, 50 - len(title))}")
    for s in signals:
        if isinstance(s, tuple):
            print(f"  {s[0]} {s[1]}")
        elif isinstance(s, dict):
            print(f"  {s['Level']}  {s['Signal']}")
            print(f"         â†’ {s['Implication']}")
            if s.get('Data'):
                print(f"         ğŸ“Š {s['Data']}")
            print()


# ============================================================
# ë©”ì¸ ëŒ€ì‹œë³´ë“œ (v2.0)
# ============================================================

def run_dashboard(detail=False, export=False):
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ ì‹¤í–‰."""

    # ë°°ì¹˜ ìˆ˜ì§‘
    category_data, errors, meta = fetch_all_data(days=400)

    if not category_data:
        print("  âš ï¸  ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨. ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # í•„ìˆ˜ í‹°ì»¤ í™•ì¸
    fetched_set = set()
    for df in category_data.values():
        fetched_set.update(df['Ticker'].values)
    missing = check_required_tickers(fetched_set)
    print_missing_warning(missing)

    # ê·¸ë£¹ ì„œë¨¸ë¦¬ ê³„ì‚°
    summaries = compute_group_summaries(category_data)

    # í—¤ë”
    print_header("ê¸€ë¡œë²Œ í”Œë¡œìš° ëª¨ë‹ˆí„° v2.0 â€” íˆíŠ¸ë§µ ëŒ€ì‹œë³´ë“œ", meta.get('last_date'))

    # ì„œë¨¸ë¦¬ ë˜ëŠ” ë””í…Œì¼
    if detail:
        print_detail_tables(category_data)
    else:
        print(f"\nâ”€â”€ ğŸ“Š ê·¸ë£¹ í‰ê·  ì„œë¨¸ë¦¬ {'â”€' * 40}")
        print_group_summary(summaries)
        print(f"\n  â„¹ï¸  ê°œë³„ í‹°ì»¤ í™•ì¸: --detail í”Œë˜ê·¸ ì‚¬ìš©")

    # ë¡œí…Œì´ì…˜ ìŠ¤ì½”ì–´
    rot_result = compute_rotation_score(category_data)
    if rot_result is not None:
        rot_score, score_meta = rot_result
        direction = "â†’ Growthâ†’Value ì§„í–‰" if rot_score > 0 else "â†’ Valueâ†’Growth ë³µê·€"
        bar = "â–ˆ" * min(abs(int(rot_score)), 20)
        print(f"\n  ğŸ“Š ë¡œí…Œì´ì…˜ ìŠ¤ì½”ì–´ (1W): {rot_score:+.1f}  {direction}")
        print(f"     [{bar}]")
        if score_meta is not None:
            raw = score_meta['raw_with_leverage']
            dev = score_meta['deviation_pct']
            print(f"     ê²€ì¦: ë ˆë²„ë¦¬ì§€ í¬í•¨ ì‹œ {raw:+.1f} (í¸ì°¨ {dev:.0f}%)")

    # EM ìƒëŒ€ ì„±ê³¼
    em_rel = compute_em_relative(category_data)
    if em_rel is not None:
        print(f"\nâ”€â”€ í•œêµ­ ìƒëŒ€ ì„±ê³¼ {'â”€' * 40}")
        print(tabulate(em_rel, headers='keys', tablefmt='simple', showindex=False))

    # ë¦¬ìŠ¤í¬ ì‹œê·¸ë„
    risk_signals = compute_risk_dashboard(category_data)
    if risk_signals:
        print_signals("ë¦¬ìŠ¤í¬ ìƒíƒœ íŒë‹¨", risk_signals)

    # ë³µí•© ì‹œê·¸ë„
    composite = generate_composite_signals(category_data)
    print_signals("ë³µí•© ì‹œê·¸ë„ â€” ì™¸ì¸ í–‰ë™ ì˜ˆì¸¡", composite)

    # ê·¸ë£¹ ê°„ Transfer Entropy (4ê°œ ìœˆë„ìš° Ã— ê°€ê²©/í”Œë¡œìš°)
    group_te_price = {}
    group_te_flow = {}
    close_series = meta.get('close_series', {})
    volume_series = meta.get('volume_series', {})
    te_windows = {'2W': 10, '1M': 21, '3M': 63, '6M': 126}

    if close_series:
        grp_ret_full = build_group_returns(close_series, window=300)
        grp_flow_full = build_group_flow_returns(close_series, volume_series, window=300) if volume_series else pd.DataFrame()

        # ê°€ê²© TE
        print(f"\nâ”€â”€ ê·¸ë£¹ ê°„ ì •ë³´ íë¦„: ê°€ê²© TE (Price) {'â”€' * 22}")
        if len(grp_ret_full.columns) >= 3:
            for label, window in te_windows.items():
                grp_ret = grp_ret_full.iloc[-window:] if len(grp_ret_full) >= window else grp_ret_full
                n_obs = len(grp_ret)
                if n_obs < 10:
                    continue
                te_bins = 3 if n_obs < 15 else 4 if n_obs < 30 else 6 if n_obs < 60 else 8
                print(f"  [{label}] {n_obs}ì¼ (bins={te_bins}) â†’ ê³„ì‚° ì¤‘...")
                te_result = compute_group_te(grp_ret, bins=te_bins, max_lag=2, n_surrogates=50, top_n=8)
                group_te_price[label] = te_result
                if te_result:
                    for te in te_result[:3]:
                        print(f"    {te['direction']:30s} Z={te['net_z']:+.1f} lag={te['lag']}")

        # í”Œë¡œìš° TE
        print(f"\nâ”€â”€ ê·¸ë£¹ ê°„ ì •ë³´ íë¦„: í”Œë¡œìš° TE (Dollar Volume) {'â”€' * 14}")
        if len(grp_flow_full.columns) >= 3:
            for label, window in te_windows.items():
                grp_flow = grp_flow_full.iloc[-window:] if len(grp_flow_full) >= window else grp_flow_full
                n_obs = len(grp_flow)
                if n_obs < 10:
                    continue
                te_bins = 3 if n_obs < 15 else 4 if n_obs < 30 else 6 if n_obs < 60 else 8
                print(f"  [{label}] {n_obs}ì¼ (bins={te_bins}) â†’ ê³„ì‚° ì¤‘...")
                te_result = compute_group_te(grp_flow, bins=te_bins, max_lag=2, n_surrogates=50, top_n=8)
                group_te_flow[label] = te_result
                if te_result:
                    for te in te_result[:3]:
                        print(f"    {te['direction']:30s} Z={te['net_z']:+.1f} lag={te['lag']}")
        else:
            print("  Volume ë°ì´í„° ë¶€ì¡± â€” ê±´ë„ˆëœ€")

    # ë§¤ìˆ˜ íš¨ìœ¨
    buying_eff = None
    buying_eff = compute_buying_efficiency()
    if buying_eff is not None:
        print(f"\nâ”€â”€ ê°œì¸ ë§¤ìˆ˜ íš¨ìœ¨ (Buying Efficiency) {'â”€' * 22}")
        if buying_eff.get('error'):
            print(f"  âš ï¸  {buying_eff['error']}")
        else:
            kospi_chg = buying_eff['kospi_change']
            ind_net = buying_eff['individual_net_buy']
            eff = buying_eff['efficiency']
            fgn_net = buying_eff.get('foreign_net_buy')

            chg_color = "â–²" if kospi_chg > 0 else "â–¼" if kospi_chg < 0 else "â”€"
            print(f"  ğŸ“… ê¸°ì¤€ì¼: {buying_eff['date']}")
            print(f"  ğŸ“Š ì½”ìŠ¤í”¼: {buying_eff['kospi_close']:,.0f} ({chg_color}{kospi_chg:+.1f}p)")
            print(f"  ğŸ§‘ ê°œì¸ ìˆœë§¤ìˆ˜: {ind_net:+.2f}ì¡°ì›")
            if fgn_net is not None:
                print(f"  ğŸŒ ì™¸ì¸ ìˆœë§¤ìˆ˜: {fgn_net:+.2f}ì¡°ì›")
            if eff is not None:
                if eff > 0:
                    label = "ê°œì¸ ë§¤ìˆ˜ â†’ ì§€ìˆ˜ ìƒìŠ¹ (íš¨ìœ¨ì )"
                elif eff > -5:
                    label = "ê°œì¸ ë§¤ìˆ˜ â†’ ì™¸ì¸ ë§¤ë„ ìƒì‡„ ì¤‘"
                else:
                    label = "ê°œì¸ ë§¤ìˆ˜ì—ë„ ì§€ìˆ˜ í•˜ë½ â€” ë°©ì–´ ì‹¤íŒ¨"
                print(f"  ğŸ’¡ ë§¤ìˆ˜ íš¨ìœ¨: {eff:+.1f} ({label})")
            else:
                print(f"  ğŸ’¡ ë§¤ìˆ˜ íš¨ìœ¨: ê³„ì‚° ë¶ˆê°€ (ìˆœë§¤ìˆ˜ â‰ˆ 0)")
    elif not HAS_PYKRX:
        print(f"\nâ”€â”€ ê°œì¸ ë§¤ìˆ˜ íš¨ìœ¨ {'â”€' * 33}")
        print(f"  â„¹ï¸  pykrx ë¯¸ì„¤ì¹˜. 'pip install pykrx'ë¡œ ì„¤ì¹˜ ì‹œ ìë™ í™œì„±í™”.")

    # ì—ëŸ¬
    if errors:
        print(f"\n  âš ï¸  ìˆ˜ì§‘ ì‹¤íŒ¨ ({len(errors)}ê°œ): {', '.join(str(e) for e in errors[:10])}")

    # ë‚´ë³´ë‚´ê¸°
    if export:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        for cat_key, df in category_data.items():
            filename = f"flow_monitor_{cat_key}_{timestamp}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"  ğŸ’¾ CSV: {filename}")
        group_te_combined = {'price': group_te_price, 'flow': group_te_flow}
        export_json_v2(category_data, summaries, composite, meta, buying_eff, group_te=group_te_combined)

    # í‘¸í„°
    if meta.get('last_date'):
        ld = meta['last_date']
        weekdays_ko = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        wd = weekdays_ko[ld.weekday()]
        date_note = f"  ë°ì´í„° ê¸°ì¤€: {ld.strftime('%Y-%m-%d')} ({wd})"
    else:
        date_note = "  ë°ì´í„° ê¸°ì¤€: ìˆ˜ì§‘ ì‹¤íŒ¨"

    print("\n" + "â•" * 70)
    print(date_note)
    print(f"  ì´ {len(fetched_set)}ê°œ í‹°ì»¤ ìˆ˜ì§‘ ì™„ë£Œ")
    print("  ë°ì´í„° ì†ŒìŠ¤: Yahoo Finance (15ë¶„ ì§€ì—°)" +
          (" + KRX (pykrx)" if HAS_PYKRX else ""))
    print("â•" * 70 + "\n")


# ============================================================
# ì§„ì…ì 
# ============================================================

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ê¸€ë¡œë²Œ í”Œë¡œìš° ëª¨ë‹ˆí„° v2.0')
    parser.add_argument('--detail', action='store_true', help='ê°œë³„ í‹°ì»¤ ì „ì²´ ì¶œë ¥')
    parser.add_argument('--no-export', action='store_true', help='JSON/CSV ë‚´ë³´ë‚´ê¸° ìƒëµ')

    args = parser.parse_args()
    run_dashboard(detail=args.detail, export=not args.no_export)

    # export ì‹œ standalone HTML ìë™ ë¹Œë“œ (file://ë¡œ ë°”ë¡œ ì—´ë¦¼)
    if not args.no_export:
        try:
            from build_standalone import build_standalone
            out = build_standalone()
            print(f"  ğŸŒ standalone HTML: {out} (ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì—´ê¸° ê°€ëŠ¥)")
        except Exception as e:
            print(f"  âš ï¸  standalone ë¹Œë“œ ì‹¤íŒ¨: {e}")
